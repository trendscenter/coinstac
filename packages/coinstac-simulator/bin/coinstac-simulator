#!/usr/bin/env node
const util = require('util');
const readFile = util.promisify(require('fs').readFile);
const program = require('commander');
const path = require('path');
const winston = require('winston');
const fs = require('fs');
const parseJson = require('json-parse-even-better-errors');
const termkit = require('terminal-kit');
const term = require('terminal-kit').terminal;

const pkg = require('../package.json');

term.on('key', (name) => {
  if (name === 'CTRL_C') { process.exit(); }
});

const logger = winston.createLogger({
  level: 'debug',
  transports: [
    new winston.transports.Console({ format: winston.format.cli() }),
  ],
});

let DEBUG_MODE = process.execArgv.includes('--inspect-brk=9229');

process.on('unhandledRejection', (error) => {
  logger.error(`unhandledRejection: ${error.stack}`); // eslint-disable-line no-console
  process.exit(1);
});

program
  .version(pkg.version)
  .option('-i, --inputspec [path]', '/path/to/inputSpec, defaults to ./test/inputspec.json')
  .option('-s, --compspec [path]', '/path/to/compSpec optional, defaults to cwd')
  .option('-c, --clientNumber [number]', 'number of clients, defaults to 1 not included the remote in decentralized mode. In local mode this creates separate runs')
  .option('-d, --directory [path]', ' path to where input, output, and cache is stored. Defaults to ./test/[input|cache|output] or ./coinstac for preprocessing')
  .option('-t, --showinput', ' on errors show comp input that computation failed with')
  .option('-e, --debug', ' enter debug mode, displays extra info including input')
  .option('-p, --pipeline [path]', ' give a pipeline file to run a full pipeline from, working dir used is first comp listed')
  .option('-u, --upload', ' upload computation spec')
  .option('--username [username]', ' username for uploads')
  .option('--password [password]', ' password for uploads')
  .option('--loglevel [level]', ' enter info, debug, and silly log levels')
  .option('--service [service]', 'container service, either singularity or docker, defaults to docker')
  .option('--preprocess', 'preprocess');

program.on('--help', () => {
  logger.info([ // eslint-disable-line
    '\n',
    '\tTest a computation in a COINSTAC simulated environment.\n\n',
    '\tThe simulator is meant to run in your computation\'s directory',
    '\tand requires a valid compspec and with the specified docker image\n',
    '\tin your docker image library.\n\n',
    '\tIf you do not have an input map for your computation, a prompt will.\n',
    '\tguide you through creating one.\n\n',
    '\tFurther help can be found in the Readme:\n',
    '\thttps://github.com/MRN-Code/coinstac/blob/master/packages/coinstac-simulator/README.md\n\n',
    '\tExample:\n\n',
    '\t  coinstac-sim -i ./inputSpec.json -c 2\n',
  ].join(' '));
});

program.parse(process.argv);

let globalMode = 'local';
const inputSpecPath = program.directory ? path.join(program.directory, 'inputspec.json') : path.join('.', 'test', 'inputspec.json');

if (program.loglevel) {
  DEBUG_MODE = true;
  process.LOGLEVEL = program.loglevel;
  logger.level = process.LOGLEVEL;
} else if (DEBUG_MODE) {
  process.LOGLEVEL = 'debug';
  logger.level = 'debug';
}
// require after LOGLEVEL set
const sim = require('../src/');
const compUpload = require('../src/comspec-upload');
const { pullImages, getStats, setServiceProvider } = require('coinstac-container-manager'); // eslint-disable-line import/order
const { fetchPreprocessComputations, createInputMap, prepareDirectory } = require('../src/preprocess');
setServiceProvider(program.service || 'docker');

async function runPipeline(pipelineSpec, operatingDirectory) {
  /* eslint-disable no-console */
  const count = program.clientNumber ? program.clientNumber // eslint-disable-line no-nested-ternary, max-len
    : (Array.isArray(pipelineSpec) ? pipelineSpec.length : 1); // eslint-disable-line no-nested-ternary, max-len
  logger.debug(`Starting ${globalMode} simulator run with ${count} client(s)`);

  const started = sim.startRun({
    spec: pipelineSpec,
    runMode: globalMode,
    clientCount: count,
    operatingDirectory: operatingDirectory || program.directory,
    service: program.service || 'docker',
  });
  try {
    const simRun = await started;

    const rowKeys = [
      'pipeline\nName',
      'current\nIteration',
      'controllerState',
      'pipeline\nStep',
      'mode',
      'waitingOn',
      'containerId',
      'name',
      'memUsageLimit',
      'memPercent',
      'cpuPercent',
    ];
    let textTable;
    if (!DEBUG_MODE) {
      const document = term.createDocument({
        palette: new termkit.Palette(),
      });
      textTable = new termkit.TextTable({
        parent: document,
        cellContents: [
          rowKeys,
          ['', '', '', '', '', '', '', '', '', '', ''],
          ['', '', '', '', '', '', '', '', '', '', ''],
          ['', '', '', '', '', '', '', '', '', '', ''],
          ['', '', '', '', '', '', '', '', '', '', ''],
          ['', '', '', '', '', '', '', '', '', '', ''],
          ['', '', '', '', '', '', '', '', '', '', ''],
          ['', '', '', '', '', '', '', '', '', '', ''],
          ['', '', '', '', '', '', '', '', '', '', ''],
          ['', '', '', '', '', '', '', '', '', '', ''],
          ['', '', '', '', '', '', '', '', '', '', ''],
        ],
        contentHasMarkup: true,
        fit: true,
        borderAttr: { color: 'blue' },
        selectable: 'cell',
        autoWidth: 1,
        autoHeight: 1,
      });
    }
    const writeValuesToRow = (rowNumber, valuesToWrite) => {
      if (DEBUG_MODE) return;
      Object.keys(valuesToWrite).forEach((rowKey) => {
        textTable.setCellContent(rowKeys.indexOf(rowKey), rowNumber, valuesToWrite[rowKey]);
      });
    };

    const localPipelines = simRun.pipelines.locals;
    const remotePipeline = simRun.pipelines.remote;

    const formatUpdateEvent = (data) => {
      const {
        currentIteration,
        controllerState,
        pipelineStep,
        mode,
        totalSteps,
        waitingOn,
      } = data;
      return {
        'current\nIteration': currentIteration,
        controllerState,
        'pipeline\nStep': `${pipelineStep}/${totalSteps}`,
        mode,
        waitingOn,
      };
    };
    // hook into the pipeline events
    if (remotePipeline) {
      const remoteStateEmitter = remotePipeline.pipeline.stateEmitter;
      remoteStateEmitter.on('update', (m) => {
        writeValuesToRow(1, formatUpdateEvent(m));
      });
    }

    localPipelines.forEach((pipeline, index) => {
      const { stateEmitter } = pipeline.pipeline;
      stateEmitter.on('update', (m) => {
        writeValuesToRow(index + 2, formatUpdateEvent(m));
      });
    });

    const containerStatsInterval = setInterval(async () => {
      const formatContainerStats = (containerStatObject) => {
        if (!containerStatObject) {
          return {
            containerId: null,
            name: null,
            memUsage: null,
            memLimit: null,
            memPercent: null,
            cpuPercent: null,
          };
        }
        const { name } = containerStatObject;
        // truncate this to the last 12 characters of the id
        const containerId = containerStatObject.id.substring(containerStatObject.id.length - 12);
        const memUsageLimit = `${((containerStatObject.memory_stats.usage) / 1000000).toFixed(3)}MB/${(containerStatObject.memory_stats.limit / 1000000).toFixed(3)}MB`;
        // truncate this to a few decimal places
        const memPercent = (
          containerStatObject.memory_stats.usage / containerStatObject.memory_stats.limit
        ).toFixed(3);

        function getCpuPercent(containerStatObject) {
          const currentCpuUsage = containerStatObject.cpu_stats.cpu_usage.total_usage;
          const preCpuUsage = containerStatObject.precpu_stats.cpu_usage.total_usage;
          const cpuDelta = currentCpuUsage - preCpuUsage;
          const systemUsage = containerStatObject.cpu_stats.system_cpu_usage;
          const preSystemUsage = containerStatObject.precpu_stats.system_cpu_usage;
          const systemDelta = systemUsage - preSystemUsage;

          try {
            const cpuPercent = (cpuDelta / systemDelta)
              * (containerStatObject.cpu_stats.cpu_usage.percpu_usage
                ? containerStatObject.cpu_stats.cpu_usage.percpu_usage.length
                : containerStatObject.cpu_stats.online_cpus) * 100.0;
            // truncate this to a few decmial places
            return (cpuPercent).toFixed(3);
          } catch (e) {
            return null;
          }
        }

        const cpuPercent = getCpuPercent(containerStatObject);

        const formatted = {
          containerId,
          name,
          memUsageLimit,
          memPercent,
          cpuPercent,
        };

        return formatted;
      };

      if (remotePipeline) {
        const remotePipelineStats = await getStats('simulatorRun', 'remote');
        const formattedRemotePipelineStats = formatContainerStats(remotePipelineStats);
        writeValuesToRow(1, { 'pipeline\nName': 'remote', ...formattedRemotePipelineStats });
      }

      const localPipelineStatsPromises = [];

      localPipelines.forEach((pipeline, index) => {
        localPipelineStatsPromises.push(getStats('simulatorRun', `local${index}`));
      });

      const localPipelinesStats = await Promise.all(localPipelineStatsPromises);

      const formattedLocalPipelinesStats = [];

      localPipelinesStats.forEach((containerStatObject) => {
        const formatted = formatContainerStats(containerStatObject);
        formattedLocalPipelinesStats.push(formatted);
      });

      formattedLocalPipelinesStats.forEach((formattedContainerStat, index) => {
        writeValuesToRow(index + 2, { 'pipeline\nName': `local${index}`, ...formattedContainerStat });
      });
    }, 1000);

    const results = await simRun.allResults;
    clearInterval(containerStatsInterval);

    logger.info('Simulator run finished');
    if (globalMode === 'decentralized') {
      logger.info(`Decentralized results:\n ${JSON.stringify(results.locals[0], null, 2)}`);
    } else {
      logger.info('Local run(s) finished');
      results.locals.forEach((res, idx) => logger.info(`Client ${idx} results:\n${JSON.stringify(res, null, 2)}\n`));
    }
  } catch (err) {
    logger.error('Simulator run failed!');
    logger.error('Fun error details:\n');
    logger.error(err.message);
    logger.error(err.stack || '');
  }
  return started;
}

function pipelineSpecFactory(inputSpec, mode, comp) {
  let specMap = Array.isArray(inputSpec) ? inputSpec : [inputSpec];
  const comps = Array.isArray(comp) ? comp : [comp];
  specMap = specMap.map((spec, index) => ({
    inputMap: spec,
    controller: { type: mode[index] },
    computations: [comps[index]],
  }));
  return {
    steps: specMap,
  };
}

const startPreprocessRun = async (pipelineSpec, baseDir) => {
  await fs.promises.rm(path.join(baseDir, 'input'), { recursive: true, force: true });
  await prepareDirectory(pipelineSpec, baseDir);
  const run = await runPipeline(pipelineSpec, baseDir);

  // cleanup input dir
  await fs.promises.rm(path.join(baseDir, 'input'), { recursive: true, force: true });

  //  move dir to better store results
  const originalDir = path.join(baseDir, 'output', 'local0', 'simulatorRun');
  const destinationDir = path.join(baseDir, 'output', `preprocessedData-${new Date().toLocaleString('en-US').replace(/[ ,:/]/ig, '-')}`);

  await fs.promises.rename(
    originalDir,
    destinationDir
  );

  return run;
};

const upload = async () => {
  if (!program.username && !program.password) {
    logger.error('Username and password required for compspec upload');
    process.exit(1);
  }
  logger.info('Uploading computation schema.');
  return compUpload.compspecUpload(program.username, program.password, logger);
};

const promptGenerateAndPreprocess = async () => {
  const pipelineSpecPath = './pipelineSpec.json';
  const baseDir = path.join(program.directory || '', 'coinstac/');
  let rawSpec;
  let pipelineSpec;
  let specError;
  try {
    rawSpec = fs.readFileSync(pipelineSpecPath);
  } catch (e) {
    logger.info('Pipeline spec not found, proceeding to generate one...');
    logger.info('If this is in error, make sure your spec is named \'pipelineSpec.json\'');
    rawSpec = null;
  }

  // authenticate and fetch computations
  const genSpecAndProcess = async (pipelineSpec, baseDir) => {
    if (program.username === undefined || program.password === undefined) {
      logger.error('Username and password required');
      process.exit(1);
    }
    const computations = await fetchPreprocessComputations(program.username, program.password);
    // list options of computations
    if (computations.length < 1) {
      logger.error('no computations returned from API');
      process.exit(1);
    }

    term('Select a preprocessing computation: ');
    if (pipelineSpec) term('\n\n*** Note: this will rename your current pipelineSpec and create a new one ***');
    const selectComputationAnswer = await term.singleColumnMenu(
      computations.map((computation, index) => { return `${index}) ${computation.meta.id}`; }), {}
    ).promise;

    // download the selected computation
    const selectedComputation = computations[selectComputationAnswer.selectedIndex];
    const pullStreams = await pullImages([{
      img: selectedComputation.computation.dockerImage,
      compId: selectedComputation.id,
      compName: selectedComputation.meta.name,
    }]);

    logger.info('Downloading docker image...');
    await Promise.all(
      pullStreams.map((obj) => {
        return new Promise((resolve, reject) => {
          if (typeof obj.stream.pipe === 'function') {
            obj.stream.pipe(process.stdout);
            obj.stream
              .on('end', () => { resolve(); })
              .on('error', (error) => { reject(error); });
          } else {
            resolve();
            process.stdout.write('.');
          }
        });
      })
    );
    // find the .csv file
    const files = await new Promise((resolve, reject) => {
      fs.readdir('./', (err, data) => {
        if (err) {
          reject(err);
        }
        resolve(data);
      });
    });
    const csvs = files.filter((file) => {
      if (file.includes('.csv')) {
        return file;
      }
      return false;
    });
    if (csvs.length < 1) {
      logger.info('no .csv files found');
      process.exit(0);
    }

    term('Select Covariates File to use for pipeline:');
    const covariatesFileAnswer = await term.singleColumnMenu(csvs.map((csv, index) => { return `${index}) ${csv}`; })).promise;

    //  create an inputspec from the csv and the api results
    const baseInputSpec = JSON.parse(
      JSON.stringify(selectedComputation.computation.input)
    );
    const fulfilledInputMap = createInputMap(
      baseInputSpec, csvs[covariatesFileAnswer.selectedIndex]
    );
    const generatedPipeSpec = pipelineSpecFactory(fulfilledInputMap, ['local'], selectedComputation);
    if (pipelineSpec) fs.promises.rename('./pipelineSpec.json', `./pipelineSpec-${pipelineSpec.steps[0].computations[0].meta.id}-${new Date().toLocaleString('en-US').replace(/[ ,:/]/ig, '-')}.json`);
    fs.writeFileSync('pipelineSpec.json', JSON.stringify(generatedPipeSpec, null, 2));

    term('pipelineSpec.json created \n');
    term('If you do not want to use default options -\nPlease edit pipelineSpec.json manually. \n');
    term('Proceed with pipeline? y,n \n');
    const proceed = await term.yesOrNo({ yes: ['y'], no: ['n'] }).promise;

    if (proceed) {
      return startPreprocessRun(generatedPipeSpec, baseDir);
    }
    process.exit();
  };

  if (rawSpec) {
    try {
      pipelineSpec = parseJson(rawSpec);
    } catch (e) {
      specError = `Your pipelineSpec's JSON formatted incorrectly
      please edit it to fix any errors:
      ${e}
      Do you want to generate a new one?`;
    }
  }

  if (!pipelineSpec && !specError) {
    await genSpecAndProcess(pipelineSpec, baseDir);
  } else if (specError) {
    term(specError);
    const newSpec = await term.yesOrNo({ yes: ['ENTER', 'y'], no: ['n'] }).promise;
    if (newSpec) {
      await genSpecAndProcess(pipelineSpec, baseDir);
    }
  } else {
    term(`Preprocess using existing pipeline for ${pipelineSpec.steps[0].computations[0].meta.name}? y,n \n`);
    const useExistingPipeline = await term.yesOrNo({ yes: ['y'], no: ['n'] }).promise;
    if (useExistingPipeline) {
      await startPreprocessRun(pipelineSpec, baseDir);
    } else {
      await genSpecAndProcess(pipelineSpec, baseDir);
    }
  }
};

const pipelineSpecFulfill = async () => {
  const compspec = JSON.parse(await readFile(program.compspec ? program.compspec : 'compspec.json'));
  const inputSpec = JSON.parse(
    await readFile(program.inputspec ? program.inputspec : inputSpecPath)
  );
  globalMode = compspec.computation.remote ? 'decentralized' : 'local';
  const mode = new Array(2).fill(globalMode);

  // TODO: create an inputSpec if one doesn't exist

  let generatedPipeSpec;
  if (Array.isArray(inputSpec)) {
    generatedPipeSpec = inputSpec.map(spec => pipelineSpecFactory(spec, mode, compspec));
  } else {
    generatedPipeSpec = pipelineSpecFactory(inputSpec, mode, compspec);
  }
  return generatedPipeSpec;
};

const pipelineSpecFulfillPipelineOption = async () => {
  const pipeSpec = await readFile(program.pipeline);

  const compSpecs = JSON.parse(pipeSpec);
  const fileProm = compSpecs.reduce((memo, spec, index) => {
    const baseDir = path.dirname(path.resolve(path.dirname(program.pipeline), spec));
    memo.push(Promise.all([
      readFile(path.resolve(baseDir, spec))
        .then(file => JSON.parse(file)).catch(() => {
          throw new Error(`Incorrect json or missing file at ${path.resolve(baseDir, spec)}`);
        }),
      readFile(path.join(baseDir, 'test', index === 0 ? 'inputspec.json' : 'inputspec-pipeline.json'))
        .then(file => JSON.parse(file)).catch(() => {
          throw new Error(`Incorrect json or missing file at ${path.join(baseDir, 'test', index === 0 ? 'inputspec.json' : 'inputspec-pipeline.json')}`);
        }),
    ]));
    return memo;
  }, []);

  const fileGroups = await fileProm;
  // each group element is [0] compspec, [1] inputSpec
  // group compspecs and inputspecs into an object we can create pipepecs from
  const mode = fileGroups.reduce((memo, fileGroup) => {
    if (fileGroup[0].computation.remote) {
      memo.push('decentralized');
      globalMode = 'decentralized';
    } else {
      memo.push('local');
    }
    return memo;
  }, []);

  let clientCount;
  const groupedSpecs = fileGroups.reduce((memo, fileGroup, index) => {
    memo.compSpecs.push(fileGroup[0]);
    const inputSpec = Array.isArray(fileGroup[1]) ? fileGroup[1] : [fileGroup[1]];

    clientCount = clientCount || inputSpec.length;
    if (clientCount !== inputSpec.length) throw new Error('Mismached inputspec client count');

    inputSpec.forEach((spec, idx) => {
      if (memo.inputSpecs[idx]) {
        memo.inputSpecs[idx][index] = spec;
      } else {
        memo.inputSpecs[idx] = [spec];
      }
    });

    return memo;
  }, { compSpecs: [], inputSpecs: [] });

  const pipelineSpecs = groupedSpecs.inputSpecs.map((site) => {
    return pipelineSpecFactory(site, mode, groupedSpecs.compSpecs);
  });

  return pipelineSpecs;
};

if (program.preprocess) {
  (async () => {
    try {
      await promptGenerateAndPreprocess();
      process.exit();
    } catch (error) {
      logger.error(error.message);
      process.exit(1);
    }
  })();
} else if (program.upload) {
  upload()
    .then(() => {
      process.exit();
    })
    .catch((e) => {
      logger.error(e.message);
      process.exit(1);
    });
} else {
  let fulfilledPipelineSpec;

  if (program.pipeline) {
    fulfilledPipelineSpec = pipelineSpecFulfillPipelineOption();
  } else {
    fulfilledPipelineSpec = pipelineSpecFulfill();
  }

  fulfilledPipelineSpec
    .then(async (pipelineSpec) => {
      await runPipeline(pipelineSpec);
      process.exit(0);
    });
}
